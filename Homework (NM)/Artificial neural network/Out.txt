   x     f(x)  f'(x)  f''(x)  ∫f(x)dx
-1.000  -0.056  -0.253  -10.798  -0.004
-0.950  -0.052  0.444  -13.760  -0.007
-0.900  -0.018  0.924  -60.624  -0.009
-0.850  0.006  0.229  81.270  -0.009
-0.800  -0.017  -0.957  17.487  -0.009
-0.750  -0.082  -1.572  7.822  -0.011
-0.700  -0.169  -1.883  6.012  -0.017
-0.650  -0.271  -2.241  8.287  -0.028
-0.600  -0.394  -2.665  7.932  -0.045
-0.550  -0.536  -2.961  3.210  -0.068
-0.500  -0.684  -2.922  -5.287  -0.098
-0.450  -0.820  -2.391  -16.236  -0.136
-0.400  -0.914  -1.294  -27.454  -0.180
-0.350  -0.940  0.311  -36.056  -0.226
-0.300  -0.878  2.220  -39.218  -0.272
-0.250  -0.719  4.112  -35.176  -0.313
-0.200  -0.473  5.617  -23.940  -0.343
-0.150  -0.169  6.413  -7.123  -0.359
-0.100  0.152  6.269  13.345  -0.359
-0.050  0.440  5.053  35.068  -0.344
0.000  0.642  2.899  48.057  -0.317
0.050  0.729  0.743  32.056  -0.282
0.100  0.743  0.146  -8.762  -0.245
0.150  0.772  1.092  -18.982  -0.207
0.200  0.835  1.058  25.725  -0.167
0.250  0.837  -1.238  56.975  -0.125
0.300  0.710  -3.594  29.517  -0.086
0.350  0.512  -3.984  -10.856  -0.055
0.400  0.335  -3.010  -23.254  -0.034
0.450  0.212  -1.963  -17.452  -0.021
0.500  0.133  -1.269  -10.840  -0.012
0.550  0.081  -0.828  -7.182  -0.007
0.600  0.048  -0.527  -4.981  -0.004
0.650  0.027  -0.320  -3.360  -0.002
0.700  0.014  -0.181  -2.379  -0.001
0.750  0.008  -0.076  -1.465  0.000
0.800  0.005  -0.085  1.222  0.000
0.850  0.000  -0.054  -2.368  0.000
0.900  0.000  0.013  0.077  0.000
0.950  0.000  -0.002  0.138  0.000
1.000  0.000  -0.002  -0.034  0.000

--- Target Function ---
f(x) = cos(5x - 1) * exp(-x^2)

--- Conclusion ---
The approximation varies with every run. Sometimes it is very good, sometimes it is not.
Most often, the neural network with 8 hidden neurons approximates the target function and its derivatives,
but the values, particularly near some or all peaks, are noticeably underestimated compared to the expected magnitude.
This suggests the model captures the general shape but struggles with accurately fitting the higher-amplitude features.

Improving the approximation may require careful tuning of training parameters such as the learning rate and initialization strategy.
Increasing the number of hidden neurons might enhance the network’s capacity but can also introduce instability,
potentially causing NaN values or divergence without proper regularization or training adjustments.
Adding stricter parameter constraints and refining the cost function can help maintain stable optimization.

Overall, while the current network mostly captures the qualitative behavior, further work is needed to improve precision
at critical points and ensure stable convergence.
